{\rtf1\ansi\ansicpg1252\cocoartf2867
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import torch\
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\
\
MERGED_PATH = "/home/Fine Tuning Files/merged_model"\
\
# Load tokenizer and model\
print("Loading model...")\
tokenizer = AutoTokenizer.from_pretrained(MERGED_PATH)\
model = AutoModelForCausalLM.from_pretrained(\
    MERGED_PATH,\
    torch_dtype=torch.bfloat16,\
    device_map="auto",\
)\
\
# Create pipeline\
pipe = pipeline(\
    "text-generation",\
    model=model,\
    tokenizer=tokenizer,\
)\
\
# Test question\
messages = [\
    \{"role": "system", "content": "You are a knowledgeable and compassionate medical assistant."\},\
    \{"role": "user",   "content": "What are the symptoms of Type 2 Diabetes?"\},\
]\
\
print("\\n\uc0\u55357 \u56589  Asking the model a medical question...\\n")\
output = pipe(\
    messages,\
    max_new_tokens=300,\
    do_sample=True,\
    temperature=0.7,\
    top_p=0.9,\
)\
\
print("\uc0\u55358 \u56598  Model Response:")\
print(output[0]["generated_text"][-1]["content"])}